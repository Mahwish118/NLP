{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic of Text PreProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()#NLTK DOWNLOAD WILL TAKE A LOT OF TIME ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import nltk\n",
    "nltk.download()\n",
    "\n",
    " graphical interface will be presented:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](nltk-download.png)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Click all and then click download. It will download all the required packages which may take a while, the bar on the bottom shows the progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize words\n",
    "\n",
    "A sentence or data can be split into words using the method word_tokenize():"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with word_tokenize() with nltk library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent_tokenize result is     ['Happiness is when what you think what \\nyou say, and what you do are in harmnony']\n"
     ]
    }
   ],
   "source": [
    "text_1 = \"\"\"Happiness is when what you think what \n",
    "you say, and what you do are in harmnony\"\"\"\n",
    "\n",
    "from nltk import sent_tokenize,word_tokenize\n",
    "\n",
    "print(f\"sent_tokenize result is     {sent_tokenize(text)}\")\n",
    "word_sent = sent_tokenize(text_1)\n",
    "#Results\n",
    "#sent_tokenize shows the two sentence in two differnet lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word_tokenize result is     ['Happiness', 'is', 'when', 'what', 'you', 'think', 'what', 'you', 'say', ',', 'and', 'what', 'you', 'do', 'are', 'in', 'harmnony']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Word_tokenize result is     {word_tokenize(text)}\")\n",
    "tokens = word_tokenize(text_1)\n",
    "#Results\n",
    "#All of them are words except the comma. Special characters are treated as separate tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with split() -this is without nltk library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['happiness', 'is', 'when', 'what', 'you', 'think', 'what', 'you', 'say,', 'and', 'what', 'you', 'do', 'are', 'in', 'harmnony']\n"
     ]
    }
   ],
   "source": [
    "#more way without using NLTK\n",
    "text_1 = \"\"\"Happiness is when what you think what \n",
    "you say, and what you do are in harmnony\"\"\"\n",
    "#transfer the text in the lower case\n",
    "lower_text = text_1.lower()\n",
    "split_lower_text = lower_text.split()\n",
    "print(split_lower_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nltk FreqDist Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This function is used to find the frequency of words within a text. It returns a dictionary. We need to pass keys and values to get the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency Distribution of each word <FreqDist with 13 samples and 17 outcomes>\n",
      "Frequency Distribution of each word <FreqDist with 13 samples and 17 outcomes>\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "# we will use word_token result i.e b for our frequency distribution.\n",
    "fdist = FreqDist(tokens)\n",
    "\n",
    "print(f\"Frequency Distribution of each word {fdist}\")\n",
    "print(f\"Frequency Distribution of each word {FreqDist(tokens)}\")\n",
    "#total text including , is 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'what': 3, 'you': 3, 'Happiness': 1, 'is': 1, 'when': 1, 'think': 1, 'say': 1, ',': 1, 'and': 1, 'do': 1, ...})"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist\n",
    "#this will be the dictionary so it has key and value and frequency of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Happiness', 'is', 'when', 'what', 'you', 'think', 'say', ',', 'and', 'do', 'are', 'in', 'harmnony'])\n",
      "dict_values([1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(fdist.keys())\n",
    "\n",
    "print(fdist.values())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Stopwords?\n",
    "\n",
    "Stopwords are the most common words in any natural language. For the purpose of analyzing text data and building NLP models, these stopwords might not add much value to the meaning of the document.\n",
    "\n",
    "Many sentences and paragraphs include words that have very little meaning or value. These words include “a,” “and,” “an,”,the”, “is”, “in”, “for”, “where”, “when”, “to”, “at” etc. and “the.” Stop word removal is a process of removing these words from a sentence or stream of words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we Need to Remove Stopwords?\n",
    "Quite an important question and one you must have in mind.\n",
    "\n",
    "Removing stopwords is not a hard and fast rule in NLP. It depends upon the task that we are working on. For tasks like text classification, where the text is to be classified into different categories, stopwords are removed or excluded from the given text so that more focus can be given to those words which define the meaning of the text.\n",
    "\n",
    "Advantages of removing the remove stopwords:\n",
    "\n",
    "\n",
    "1) By removing stopwords, dataset size decreases and the time to train the model also decreases.\n",
    "\n",
    "2) Removing stopwords can potentially help improve the performance as there are fewer and only meaningful tokens left. Thus, it could increase classification accuracy.\n",
    "\n",
    "\n",
    "3 Even search engines like Google remove stopwords for fast and relevant retrieval of data from the database\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Stopwords\n",
    "We can remove stopwords while performing the following tasks:\n",
    "\n",
    "Text Classification\n",
    "1)Spam Filtering\n",
    "2)Language Classification\n",
    "3)Genre Classification\n",
    "\n",
    "\n",
    "Caption Generation\n",
    "Auto-Tag Generation\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoid Stopword Removal\n",
    "Machine Translation\n",
    "\n",
    "Language Modeling\n",
    "\n",
    "Text Summarization\n",
    "\n",
    "Question-Answer problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Methods to Remove Stopwords\n",
    "Stopword Removal using NLTK\n",
    "\n",
    "NLTK, or the Natural Language Toolkit, is a treasure trove of a library for text preprocessing. It’s one of my favorite Python libraries. NLTK has a list of stopwords stored in 16 different languages.\n",
    "\n",
    "You can use the below code to see the list of stopwords in NLTK:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = list(stopwords.words('english'))\n",
    "#stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words in Arabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(stopwords.words('arabic'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To remove stopwords using NLTK, (through sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Happiness is when what you think what \\nyou say, and what you do are in harmnony']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we did it earlier \n",
    "word_sent = sent_tokenize(text_1)\n",
    "word_sent\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Happiness is when what you think what \\nyou say, and what you do are in harmnony']\n"
     ]
    }
   ],
   "source": [
    "filtered_list = []\n",
    "\n",
    "for i in word_sent:\n",
    "    if i not in stop_words:\n",
    "        filtered_list.append(i)\n",
    "        print(filtered_list)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To remove stopwords using NLTK, (through word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'red', 'fox', 'is', 'an', 'animal', 'that', 'is', 'able', 'to', 'jump', 'over', 'the', 'moon', '.']\n",
      "['red', 'fox', 'animal', 'able', 'jump', 'moon', '.']\n"
     ]
    }
   ],
   "source": [
    "#another example of stop words:\n",
    "\n",
    "#import libraries\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "\n",
    "#import text\n",
    "text_2 = 'a red fox is an animal that is able to jump over the moon.'\n",
    "#tokenize the word:\n",
    "word_tokens = word_tokenize(text_2)\n",
    "#st\n",
    "stop_words = list(stopwords.words('english'))\n",
    "\n",
    "#filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "filtered_sentence = [] \n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        filtered_sentence.append(w) \n",
    "print(filtered_sentence)\n",
    "#\"not in\"  means we are checking the string is \"not present\" in the specific text,similary \"in\"mean present "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['red', 'fox', 'animal', 'able', 'jump', 'moon', '.']"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#another way of doing above loop .\n",
    "filtered_sentence_1 = [w for w in word_tokens if w not in stop_words] \n",
    "filtered_sentence_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['redred', 'foxfox', 'animalanimal', 'ableable', 'jumpjump', 'moonmoon', '..']"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sentence_1 = [w+w for w in word_tokens if w not in stop_words] \n",
    "filtered_sentence_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'red', 'fox', 'is', 'an', 'animal', 'that', 'is', 'able', 'to', 'jump', 'over', 'the', 'moon', '.']\n",
      "['red']\n",
      "['red', 'fox']\n",
      "['red', 'fox', 'animal']\n",
      "['red', 'fox', 'animal', 'able']\n",
      "['red', 'fox', 'animal', 'able', 'jump']\n",
      "['red', 'fox', 'animal', 'able', 'jump', 'moon']\n",
      "['red', 'fox', 'animal', 'able', 'jump', 'moon', '.']\n"
     ]
    }
   ],
   "source": [
    "#another example of stop words:\n",
    "\n",
    "#import libraries\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "\n",
    "#import text\n",
    "text_2 = 'a red fox is an animal that is able to jump over the moon.'\n",
    "\n",
    "#tokenize the word:\n",
    "word_tokens = word_tokenize(text_2)\n",
    "print(word_tokens)\n",
    "#stop words for english:\n",
    "stop_words = list(stopwords.words('english'))\n",
    "\n",
    "#filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "filtered_sentence = [] \n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        filtered_sentence.append(w) \n",
    "        print(filtered_sentence)\n",
    "#\"not in\"  means we are checking the string is \"not present\" in the specific text,similary \"in\"mean present "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Text Normalization\n",
    "\n",
    "\n",
    "In any natural language, words can be written or spoken in more than one form depending on the situation. That’s what makes the language such a thrilling part of our lives, right? For example:\n",
    "\n",
    "Melissa \"ate\" the food and washed the utensils.\n",
    "\n",
    "They were \"eating\" noodles at a resturants.\n",
    "\n",
    "Don’t you want to \"eat\" before we leave?\n",
    "\n",
    "We have just \"eaten\" our breakfast.\n",
    "\n",
    "It also \"eats\" fruits and vegetables.\n",
    "\n",
    "In all these sentences, we can see that the word eat has been used in multiple forms. For us, it is easy to understand that eating is the activity here. So it doesn’t really matter to us whether it is ‘ate’, ‘eat’, or ‘eaten’ – we know what is going on.\n",
    "\n",
    "Unfortunately, that is not the case with machines. They treat these words differently. Therefore, we need to normalize them to their root word, which is “eat” in our example.\n",
    "\n",
    "Hence, text normalization is a process of transforming a word into a single canonical form(standard form). This can be done by two processes, stemming and lemmatization. Let’s understand what they are in detail.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are Stemming and Lemmatization?\n",
    "\n",
    "Stemming and Lemmatization is simply normalization of words, which means reducing a word to its root form.\n",
    "\n",
    "In most natural languages, a root word can have many variants. For example, the word ‘play’ can be used as ‘playing’, ‘played’, ‘plays’, etc. You can think of similar examples (and there are plenty).\n",
    "\n",
    "\n",
    "Steming---Stem\n",
    "lemmatization---Lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n",
    "\n",
    "Let’s first understand stemming:\n",
    "\n",
    "Stemming is a text normalization technique that cuts off the end or beginning of a word by taking into account a list of common prefixes or suffixes that could be found in that word\n",
    "It is a rudimentary rule-based process of stripping the suffixes (“ing”, “ly”, “es”, “s” etc) from a word.\n",
    "Stemming is the process of reducing noise in a word and is otherwise referred to as lexicon normalization. It reduces inflection. For example, the word “fishing” has a stem word “fish.” Stemming is used to simplify a word down to its base meaning. Another good example is the word “like” which is the stem of many words such as: “likes,” “liked,” and “likely.” Search engines use stemming for this reason. In many situations it could be useful for a search for one of these words to return documents that contain another word in the set.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To perform stemming using Python and the NLTK library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stemmer of the likes is: like\n"
     ]
    }
   ],
   "source": [
    "#####################to \n",
    "##for stemming import library\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()##is called porter stemmer method\n",
    "\n",
    "stem_1 = ps.stem(\"likes\")\n",
    "print(f\"The stemmer of the likes is: {stem_1}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stemming of likes : {'like'}\n",
      "The stemming of likely : {'like'}\n",
      "The stemming of likes : {'like'}\n",
      "The stemming of liking : {'like'}\n",
      "The stemming of trying : {'tri'}\n",
      "The stemming of tried : {'tri'}\n",
      "The stemming of try : {'tri'}\n"
     ]
    }
   ],
   "source": [
    "#STEMMING OF LIST OF WORD WITH ADVANCED PRINT OPTION\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "words = ['likes', 'likely', 'likes', 'liking','trying','tried','try']\n",
    "for w in words:\n",
    "    print(f\"The stemming of\", w, \":\" ,{ps.stem(w)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "\n",
    "\n",
    "Lemmatization, on the other hand, is an organized & step-by-step procedure of obtaining the root form of the word. It makes use of vocabulary (dictionary importance of words) and morphological analysis (word structure and grammar relations).\n",
    "Stemming and lemmatization are very similar in that they enable you to get to the root word. This is called word normalization and both can generate the same output. However, they work very differently. Stemming attempts to chop words off where lemmatization provides you with the ability to see if the word is a noun, verb or other parts of speech. Let’s take the world “saw.” Stemming will bring back “saw” and lemmatization could bring back “see” or “saw.” Lemmatization usually brings back a readable word where stemming may not. See below for an example showing the difference.\n",
    "Let’s take a look at a Python example which compares stemming to lemmatization:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lemmitizer of likes : {'like'}\n",
      "The lemmitizer of likely : {'likely'}\n",
      "The lemmitizer of likes : {'like'}\n",
      "The lemmitizer of liking : {'liking'}\n",
      "The lemmitizer of trying : {'trying'}\n",
      "The lemmitizer of tried : {'tried'}\n",
      "The lemmitizer of try : {'try'}\n"
     ]
    }
   ],
   "source": [
    "#lemmetizer library\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()#is called lemma  method\n",
    "\n",
    "\n",
    "words = ['likes', 'likely', 'likes', 'liking','trying','tried','try']\n",
    "for w in words:\n",
    "    print(f\"The lemmitizer of\", w, \":\" ,{lemma.lemmatize(w)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and lemmetizing togethor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpora STEMMING : corpora & LEMMATIZATION : corpora\n",
      "constructing STEMMING : construct & LEMMATIZATION : construct\n",
      "better STEMMING : better & LEMMATIZATION : better\n",
      "done STEMMING : done & LEMMATIZATION : do\n",
      "worst STEMMING : worst & LEMMATIZATION : worst\n",
      "pony STEMMING : poni & LEMMATIZATION : pony\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "words = ['corpora', 'constructing', 'better', 'done', 'worst', 'pony']\n",
    "for w in words:\n",
    "     print(w, \"STEMMING :\", ps.stem(w),\"&\",\"LEMMATIZATION :\", lemmatizer.lemmatize(w, pos = \"v\"))\n",
    "#here you will notice we had used \"v\" for verb  so all lemiitization are in verb        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpora STEMMING : corpora & LEMMATIZATION : corpus\n",
      "constructing STEMMING : construct & LEMMATIZATION : constructing\n",
      "better STEMMING : better & LEMMATIZATION : better\n",
      "done STEMMING : done & LEMMATIZATION : done\n",
      "worst STEMMING : worst & LEMMATIZATION : worst\n",
      "pony STEMMING : poni & LEMMATIZATION : pony\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "ps = PorterStemmer()\n",
    "\n",
    "words = ['corpora', 'constructing', 'better', 'done', 'worst', 'pony']\n",
    "for w in words:\n",
    "     print(w, 'STEMMING :', ps.stem(w),\"&\",'LEMMATIZATION :', lemmatizer.lemmatize(w))\n",
    "# here we didnot use pro = v        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lets try the stemming and lemmetizing in the text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual text :Happiness is when what you think what \n",
      "you say, and what you do are in harmnony\n",
      "Tokenization of the text:['Happiness', 'is', 'when', 'what', 'you', 'think', 'what', 'you', 'say', ',', 'and', 'what', 'you', 'do', 'are', 'in', 'harmnony']\n",
      "Text after eliminaating stop words ['Happiness', 'think', 'say', ',', 'harmnony']\n",
      "Happiness STEMMING : happi & LEMMATIZATION : Happiness\n",
      "think STEMMING : think & LEMMATIZATION : think\n",
      "say STEMMING : say & LEMMATIZATION : say\n",
      ", STEMMING : , & LEMMATIZATION : ,\n",
      "harmnony STEMMING : harmnoni & LEMMATIZATION : harmnony\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "import nltk\n",
    "text_1 = \"\"\"Happiness is when what you think what \n",
    "you say, and what you do are in harmnony\"\"\"\n",
    "print(f\"Actual text :{text_1}\")\n",
    "######################################################\n",
    "\n",
    "from nltk import sent_tokenize,word_tokenize\n",
    "a = sent_tokenize(text_1)\n",
    "b = word_tokenize(text_1)\n",
    "print(f\"Tokenization of the text:{b}\")\n",
    "\n",
    "##########################################################\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = list(stopwords.words('english'))\n",
    "#finding stop words in the text\n",
    "filtered_list = []\n",
    "for i in b:\n",
    "    if i not in stop_words:\n",
    "        filtered_list.append(i)\n",
    "print(f\"Text after eliminaating stop words {filtered_list}\")\n",
    "\n",
    "###############################################################\n",
    "#stemming and lammitizing of the text \n",
    "        \n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "\n",
    "for w in filtered_list:\n",
    "     print(w, 'STEMMING :', ps.stem(w),\"&\",'LEMMATIZATION :', lemmatizer.lemmatize(w))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we can see we have puncutation in the result now we need remove punctuation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Happiness', 'think', 'say', ',', 'harmnony']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Happiness', 'think', 'say', 'harmnony']"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#through lambda function\n",
    "import string\n",
    "punc = string.punctuation # to see list of punctuation\n",
    "print(filtered_list)\n",
    "tokens_no_punc = list(filter(lambda token: token not in string.punctuation,filtered_list))\n",
    "tokens_no_punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Happiness', 'think', 'say', ',', 'harmnony']\n",
      "['Happiness', 'think', 'say', 'harmnony']\n"
     ]
    }
   ],
   "source": [
    "#through for loop\n",
    "import string\n",
    "punc = string.punctuation # to see list of punctuation\n",
    "print(filtered_list)\n",
    "token_without_punc = [] \n",
    "for w in filtered_list: \n",
    "    if w not in punc: \n",
    "        token_without_punc.append(w) \n",
    "print(token_without_punc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts of Speech\n",
    "\n",
    "Parts of speech is used to determine syntactic function. In the English language the main parts of speech are: adjective, pronoun, noun, verb, adverb, preposition, conjunction, and interjection. This is used to infer the intent of the word based on its use. For example the word PERMIT can be a noun and a verb. Verb use: “I permit you to go to the dance.” Noun use: “Did you get the permit from the county.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual text :Happiness is when what you think what \n",
      "you say, and what you do are in harmnony\n",
      "Tokenization of the text:['Happiness', 'is', 'when', 'what', 'you', 'think', 'what', 'you', 'say', ',', 'and', 'what', 'you', 'do', 'are', 'in', 'harmnony']\n",
      "pos_tagging of the text:[('Happiness', 'NNP'), ('is', 'VBZ'), ('when', 'WRB'), ('what', 'WP'), ('you', 'PRP'), ('think', 'VBP'), ('what', 'WP'), ('you', 'PRP'), ('say', 'VBP'), (',', ','), ('and', 'CC'), ('what', 'WP'), ('you', 'PRP'), ('do', 'VBP'), ('are', 'VBP'), ('in', 'IN'), ('harmnony', 'NN')]\n",
      "Text after eliminaating stop words ['Happiness', 'think', 'say', ',', 'harmnony']\n",
      "Happiness STEMMING : happi & LEMMATIZATION : Happiness\n",
      "think STEMMING : think & LEMMATIZATION : think\n",
      "say STEMMING : say & LEMMATIZATION : say\n",
      ", STEMMING : , & LEMMATIZATION : ,\n",
      "harmnony STEMMING : harmnoni & LEMMATIZATION : harmnony\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "import nltk\n",
    "text_1 = \"\"\"Happiness is when what you think what \n",
    "you say, and what you do are in harmnony\"\"\"\n",
    "print(f\"Actual text :{text_1}\")\n",
    "######################################################\n",
    "\n",
    "from nltk import sent_tokenize,word_tokenize\n",
    "a = sent_tokenize(text_1)\n",
    "b = word_tokenize(text_1)\n",
    "print(f\"Tokenization of the text:{b}\")\n",
    "\n",
    "##########################################################\n",
    "\n",
    "pos_text = nltk.pos_tag(b)\n",
    "print(f\"pos_tagging of the text:{pos_text}\")\n",
    "\n",
    "\n",
    "####################################################\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = list(stopwords.words('english'))\n",
    "#finding stop words in the text\n",
    "filtered_list = []\n",
    "for i in b:\n",
    "    if i not in stop_words:\n",
    "        filtered_list.append(i)\n",
    "print(f\"Text after eliminaating stop words {filtered_list}\")\n",
    "\n",
    "###############################################################\n",
    "#stemming and lammitizing of the text \n",
    "        \n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "\n",
    "for w in filtered_list:\n",
    "     print(w, 'STEMMING :', ps.stem(w),\"&\",'LEMMATIZATION :', lemmatizer.lemmatize(w))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2019/08/how-to-remove-stopwords-text-normalization-nltk-spacy-gensim-python/\n",
    "\n",
    "https://medium.com/analytics-vidhya/the-data-science-behind-natural-language-processing-69d6df06a1ff\n",
    "\n",
    "https://www.youtube.com/watch?v=3SCivTVgFZs\n",
    "\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
